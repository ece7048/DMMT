{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YIvx-8iMokKl"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "import subprocess\n",
        "from typing import Tuple, Dict\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "from torch import Tensor"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "905sFSMlv7ug"
      },
      "outputs": [],
      "source": [
        "from google.colab import drive\n",
        "drive.mount(\"/content/gdrive\")\n",
        "!ls"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EkKr2S91zMx1"
      },
      "outputs": [],
      "source": [
        "!pip install torch_geometric\n",
        "!pip install torchmetrics\n",
        "!pip install monai\n",
        "!pip install wandb\n",
        "!pip install quantus\n",
        "!pip install captum\n",
        "!pip install nibabel\n",
        "!pip install unfoldNd\n",
        "!pip install monai[einops]\n",
        "!python -c \"import monai\" || pip install -q \"monai-weekly[nibabel]\"\n",
        "!python -c \"import matplotlib\" || pip install -q matplotlib\n",
        "%matplotlib inline"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "import json\n",
        "import shutil\n",
        "import tempfile\n",
        "import time\n",
        "\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "import nibabel as nib\n",
        "\n",
        "from monai.losses import DiceLoss,SSIMLoss\n",
        "#from monai.inferers import sliding_window_inference\n",
        "from monai import transforms\n",
        "from monai.transforms import (\n",
        "    AsDiscrete,\n",
        "    Activations,\n",
        ")\n",
        "\n",
        "from monai.config import print_config\n",
        "from monai.metrics import DiceMetric,ROCAUCMetric,HausdorffDistanceMetric,ConfusionMatrixMetric,MSEMetric,MultiScaleSSIMMetric\n",
        "from monai.utils.enums import MetricReduction\n",
        "from monai.networks.nets import SwinUNETR\n",
        "from monai import data\n",
        "from monai.data import decollate_batch\n",
        "from functools import partial\n",
        "\n",
        "import torch\n",
        "\n",
        "\n",
        "print_config()"
      ],
      "metadata": {
        "id": "sSyPYysI7Sd0"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "import os\n",
        "import torch\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "from torchdata.datapipes.iter import IterableWrapper\n",
        "from torchvision.transforms import InterpolationMode\n",
        "import nibabel as nib\n",
        "import numpy as np\n",
        "from scipy import ndimage\n",
        "import skimage.transform as skTrans\n",
        "import csv\n",
        "import quantus\n",
        "from torch.nn.functional import one_hot\n",
        "\n",
        "from monai.transforms import (\n",
        "    EnsureChannelFirstd,\n",
        "    AsDiscrete,\n",
        "    Compose,\n",
        "    CropForegroundd,\n",
        "    EnsureTyped,\n",
        "    FgBgToIndicesd,\n",
        "    LoadImaged,\n",
        "    Orientationd,\n",
        "    RandCropByPosNegLabeld,\n",
        "    ScaleIntensityRanged,\n",
        "    Spacingd,\n",
        "    RandFlip,\n",
        "    RandRotate,\n",
        "    RandZoom,\n",
        "    RandAffineGrid,\n",
        "    RandGaussianNoise,\n",
        "    RandShiftIntensity,\n",
        "    NormalizeIntensity,\n",
        ")\n",
        "import torch\n",
        "import torchvision\n",
        "import torchvision.transforms as transforms\n",
        "\n",
        "\n",
        "\n",
        "class AverageMeter(object):\n",
        "    def __init__(self):\n",
        "        self.reset()\n",
        "\n",
        "    def reset(self):\n",
        "        self.val = 0\n",
        "        self.avg = 0\n",
        "        self.sum = 0\n",
        "        self.count = 0\n",
        "\n",
        "    def update(self, val, n=1):\n",
        "        self.val = val\n",
        "        self.sum += val * n\n",
        "        self.count += n\n",
        "        self.avg = np.where(self.count > 0, self.sum / self.count, self.sum)\n",
        "\n",
        "def save_checkpoint(model, epoch, filename=\"model.pt\", best_acc=0, val_loss=10000,train_loss=10000, dir_add=''):\n",
        "    state_dict = model[0].state_dict()\n",
        "    save_dict = {\"epoch\": epoch, \"best_acc\": best_acc, \"val_loss\": val_loss, \"train_loss\": train_loss,\"state_dict\": state_dict}\n",
        "    filename = os.path.join(dir_add, filename)\n",
        "    torch.save(save_dict, filename)\n",
        "    print(\"Saving checkpoint\", filename)\n",
        "\n",
        "\n",
        "transform = transforms.Compose(\n",
        "    [transforms.ToTensor(),\n",
        "     transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])\n",
        "\n",
        "\n",
        "classes = ('plane', 'car', 'bird', 'cat','deer', 'dog', 'frog', 'horse', 'ship', 'truck')\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "hqLSkwyTrKfl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LPRchmoP2B2f"
      },
      "outputs": [],
      "source": [
        "loadq2='densenet121'\n",
        "PATH= \"/content/gdrive/MyDrive/Colab/Workshop/sulcal/\"+loadq2\n",
        "PATH1= \"/content/gdrive/MyDrive/Colab/Workshop/sulcal/\""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "class DMMT(nn.Module):\n",
        "    def __init__(self,N=4,delta_k=0.02,delta_t=10):\n",
        "        super(DMMT, self).__init__()\n",
        "        self.sma = nn.AvgPool1d(kernel_size=delta_t, stride = 1)\n",
        "        self.delta_k=delta_k\n",
        "\n",
        "    def forward(self,loss,loss_prev,M,Mprev):\n",
        "        m_count=0\n",
        "        Mp=[]\n",
        "        state=False\n",
        "        for i in range(len(M)):\n",
        "            Mi=[torch.tensor(mi) for mi in M[i]]\n",
        "            Mk=torch.stack(Mi,dim=0)\n",
        "            Mcompute=Mk.view(1,1,-1)\n",
        "            Mnew=self.sma(Mcompute)\n",
        "            #print('metric sma new: ',Mnew, ' old metric: ',Mprev[i])\n",
        "            if abs(Mnew)<=(abs(Mprev[i])+(self.delta_k)) and abs(Mnew)>=(abs(Mprev[i])-(self.delta_k)):\n",
        "                m_count+=1\n",
        "            Mp.append(Mnew)\n",
        "        #print('total Metric passes: ',m_count)\n",
        "        #print('new_loss: ',loss,' old loss: ',loss_prev)\n",
        "        if m_count==len(M):\n",
        "            if loss<=loss_prev:\n",
        "                #print('DMMT converted in loss: ',loss)\n",
        "                state=True\n",
        "                return state, Mp, loss\n",
        "            else:\n",
        "                return state, Mp, loss\n",
        "        else:\n",
        "            return state, Mp, loss\n"
      ],
      "metadata": {
        "id": "exJgK95XcnC3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "MWzOt6Pbcd6w"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "def train_epoch(model, loader, optimizer, epoch, nclass,batch_size=1,max_epochs=10):\n",
        "    post_sigmoid = Activations(sigmoid=True)\n",
        "    loss_function=nn.CrossEntropyLoss()\n",
        "    start_time = time.time()\n",
        "    trun_loss = AverageMeter()\n",
        "    post_pred = Compose([Activations(softmax=True),AsDiscrete(argmax=True, to_onehot=nclass)])\n",
        "    trun_acc1 = AverageMeter()\n",
        "    trun_acc2 = AverageMeter()\n",
        "    trun_acc3 = AverageMeter()\n",
        "    trun_acc4 = AverageMeter()\n",
        "    trun_acc5 = AverageMeter()\n",
        "    trun_acc6 = AverageMeter()\n",
        "    trun_acc1.reset()\n",
        "    trun_acc2.reset()\n",
        "    trun_acc3.reset()\n",
        "    trun_acc4.reset()\n",
        "    trun_acc5.reset()\n",
        "    trun_acc6.reset()\n",
        "    roc=ROCAUCMetric(average=\"macro\")\n",
        "    f1=ConfusionMatrixMetric(metric_name=\"f1 score\",compute_sample=False,reduction=MetricReduction.MEAN_BATCH, get_not_nans=False)\n",
        "    sens=ConfusionMatrixMetric(metric_name=\"sensitivity\",compute_sample=False,reduction=MetricReduction.MEAN_BATCH, get_not_nans=False)\n",
        "    spec=ConfusionMatrixMetric(metric_name=\"specificity\",compute_sample=False,reduction=MetricReduction.MEAN_BATCH, get_not_nans=False)\n",
        "    prec=ConfusionMatrixMetric(metric_name=\"precision\",compute_sample=False,reduction=MetricReduction.MEAN_BATCH, get_not_nans=False)\n",
        "    acc=ConfusionMatrixMetric(metric_name=\"accuracy\",compute_sample=False,reduction=MetricReduction.MEAN_BATCH, get_not_nans=False)\n",
        "    post_sigmoid = Activations(sigmoid=True)\n",
        "    post_label = AsDiscrete(argmax=False, to_onehot=nclass)\n",
        "    model[0].train()\n",
        "    for idx, batch_data in enumerate(loader):\n",
        "        datad, labelsd = batch_data\n",
        "        data, labels = datad.to(device), labelsd.to(device)\n",
        "        classes = model[0](data)\n",
        "        loss_classifier = loss_function(classes,labels.long())\n",
        "        loss=loss_classifier\n",
        "        loss.backward()\n",
        "        optimizer[0].step()\n",
        "        trun_loss.update(loss.item(), n=batch_size)\n",
        "        start_time = time.time()\n",
        "\n",
        "        classes_t=classes #torch.argmax(classes,dim=1)\n",
        "        labels_t=post_label(torch.unsqueeze(labels,dim=0))\n",
        "        labels_t=torch.permute(labels_t,(1,0))\n",
        "        roc(y_pred=classes_t, y=labels_t.to(torch.int32))\n",
        "        f1(y_pred=classes_t.to(torch.int32), y=labels_t.to(torch.int32))\n",
        "        spec(y_pred=classes_t.to(torch.int32), y=labels_t.to(torch.int32))\n",
        "        sens(y_pred=classes_t.to(torch.int32), y=labels_t.to(torch.int32))\n",
        "        prec(y_pred=classes_t.to(torch.int32), y=labels_t.to(torch.int32))\n",
        "        acc(y_pred=classes_t.to(torch.int32), y=labels_t.to(torch.int32))\n",
        "        acc1 = roc.aggregate()\n",
        "        acc2_total = f1.aggregate()\n",
        "        acc3_total = acc.aggregate()\n",
        "        acc4_total = spec.aggregate()\n",
        "        acc5_total = sens.aggregate()\n",
        "        acc6_total = prec.aggregate()\n",
        "        acc2=torch.nanmean(acc2_total[0])\n",
        "        acc4=torch.nanmean(acc4_total[0])\n",
        "        acc5=torch.nanmean(acc5_total[0])\n",
        "        acc6=torch.nanmean(acc6_total[0])\n",
        "        acc3=torch.nanmean(acc3_total[0])\n",
        "        trun_acc2.update(acc2.cpu().numpy())\n",
        "        trun_acc1.update(acc1)\n",
        "        trun_acc4.update(acc4.cpu().numpy())\n",
        "        trun_acc5.update(acc5.cpu().numpy())\n",
        "        trun_acc6.update(acc6.cpu().numpy())\n",
        "        trun_acc3.update(acc3.cpu().numpy())\n",
        "\n",
        "    return trun_acc1.avg, trun_acc2.avg, trun_acc4.avg, trun_acc5.avg, trun_acc6.avg, trun_loss.avg, trun_acc3.avg\n",
        "\n",
        "\n",
        "def val_epoch(\n",
        "    model,\n",
        "    loader,\n",
        "    epoch,\n",
        "    nclass=2,\n",
        "    max_epochs=10,\n",
        "    post_label=None,\n",
        "    batch_size=1,\n",
        "):\n",
        "\n",
        "    model[0].eval()\n",
        "    start_time = time.time()\n",
        "    run_acc1 = AverageMeter()\n",
        "    run_acc2 = AverageMeter()\n",
        "    run_acc3 = AverageMeter()\n",
        "    run_acc4 = AverageMeter()\n",
        "    run_acc5 = AverageMeter()\n",
        "    run_acc6 = AverageMeter()\n",
        "    run_loss = AverageMeter()\n",
        "    run_acc1.reset()\n",
        "    run_acc2.reset()\n",
        "    run_acc3.reset()\n",
        "    run_acc4.reset()\n",
        "    run_acc5.reset()\n",
        "    run_acc6.reset()\n",
        "    loss_function=nn.CrossEntropyLoss()\n",
        "    roc=ROCAUCMetric(average=\"macro\")\n",
        "    f1=ConfusionMatrixMetric(metric_name=\"f1 score\",compute_sample=False,reduction=MetricReduction.MEAN_BATCH, get_not_nans=False)\n",
        "    sens=ConfusionMatrixMetric(metric_name=\"sensitivity\",compute_sample=False,reduction=MetricReduction.MEAN_BATCH, get_not_nans=False)\n",
        "    spec=ConfusionMatrixMetric(metric_name=\"specificity\",compute_sample=False,reduction=MetricReduction.MEAN_BATCH, get_not_nans=False)\n",
        "    prec=ConfusionMatrixMetric(metric_name=\"precision\",compute_sample=False,reduction=MetricReduction.MEAN_BATCH, get_not_nans=False)\n",
        "    acc=ConfusionMatrixMetric(metric_name=\"accuracy\",compute_sample=False,reduction=MetricReduction.MEAN_BATCH, get_not_nans=False)\n",
        "    post_sigmoid = Activations(sigmoid=True)\n",
        "    with torch.no_grad():\n",
        "        for idx, batch_data in enumerate(loader):\n",
        "            run_loss.reset()\n",
        "            labels_total=torch.tensor([])\n",
        "            classes_total=torch.tensor([])\n",
        "            datad, labelsd = batch_data\n",
        "            data,labels = datad.to(device), labelsd.to(device)\n",
        "            classes = model[0](data)\n",
        "            loss_classifier = loss_function(classes,labels.long())\n",
        "            loss=loss_classifier\n",
        "            run_loss.update(loss.item(),n=batch_size)\n",
        "            #run_acc1.reset()\n",
        "            #run_acc2.reset()\n",
        "            #run_acc3.reset()\n",
        "            #run_acc4.reset()\n",
        "            #run_acc5.reset()\n",
        "            #run_acc6.reset()\n",
        "            classes_t=classes #torch.argmax(classes,dim=1)\n",
        "            labels_t=post_label(torch.unsqueeze(labels,dim=0))\n",
        "            labels_t=torch.permute(labels_t,(1,0))\n",
        "            roc(y_pred=classes_t, y=labels_t.to(torch.int32))\n",
        "            f1(y_pred=classes_t.to(torch.int32), y=labels_t.to(torch.int32))\n",
        "            spec(y_pred=classes_t.to(torch.int32), y=labels_t.to(torch.int32))\n",
        "            sens(y_pred=classes_t.to(torch.int32), y=labels_t.to(torch.int32))\n",
        "            prec(y_pred=classes_t.to(torch.int32), y=labels_t.to(torch.int32))\n",
        "            acc(y_pred=classes_t.to(torch.int32), y=labels_t.to(torch.int32))\n",
        "            acc1 = roc.aggregate()\n",
        "            acc2_total = f1.aggregate()\n",
        "            acc3_total = acc.aggregate()\n",
        "            acc4_total = spec.aggregate()\n",
        "            acc5_total = sens.aggregate()\n",
        "            acc6_total = prec.aggregate()\n",
        "            acc2=torch.nanmean(acc2_total[0])\n",
        "            acc4=torch.nanmean(acc4_total[0])\n",
        "            acc5=torch.nanmean(acc5_total[0])\n",
        "            acc6=torch.nanmean(acc6_total[0])\n",
        "            acc3=torch.nanmean(acc3_total[0])\n",
        "            run_acc2.update(acc2.cpu().numpy())\n",
        "            run_acc1.update(acc1)\n",
        "            run_acc4.update(acc4.cpu().numpy())\n",
        "            run_acc5.update(acc5.cpu().numpy())\n",
        "            run_acc6.update(acc6.cpu().numpy())\n",
        "            run_acc3.update(acc3.cpu().numpy())\n",
        "\n",
        "\n",
        "            start_time = time.time()\n",
        "\n",
        "    return  run_acc1.avg, run_acc2.avg, run_acc4.avg, run_acc5.avg, run_acc6.avg, run_loss.avg, run_acc3.avg"
      ],
      "metadata": {
        "id": "zVqGJT1-riyM"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "def trainer(model,train_loader,val_loader,optimizer,scheduler,\n",
        "    start_epoch=0,\n",
        "    post_label=None,\n",
        "    nclass=2,\n",
        "    batch_size=1,\n",
        "    max_epochs=10,\n",
        "    model_name=\"_model.pt\",\n",
        "    root_dir=\"/content/gdrive/MyDrive/Colab/Workshop/\",\n",
        "    end_point=10,\n",
        "    LES=True,\n",
        "):\n",
        "    val_acc_max = 0.0\n",
        "    loss=1000000000000\n",
        "    val_loss_max=10000000000\n",
        "    patient=0\n",
        "    patient2=0\n",
        "    val_loss_tracker=[]\n",
        "    f1_tracker=[]\n",
        "    roc_tracker=[]\n",
        "    accur_tracker=[]\n",
        "    sens_tracker=[]\n",
        "    prec_tracker=[]\n",
        "    M_p=[0,0,0,0,0]\n",
        "    loss_p=100000000\n",
        "    loss_epochs = []\n",
        "    trains_epoch = []\n",
        "    val_every = 1\n",
        "    print(root_dir+model_name)\n",
        "    if os.path.exists(root_dir+model_name):\n",
        "        model[0].load_state_dict(torch.load(os.path.join(root_dir, model_name))[\"state_dict\"])\n",
        "        model[0].to(device)\n",
        "        checkpoint = torch.load(os.path.join(root_dir, model_name))\n",
        "        val_loss_max=checkpoint[\"val_loss\"]\n",
        "        loss=checkpoint[\"train_loss\"]\n",
        "        print('MAX acc: ', val_loss_max)\n",
        "\n",
        "\n",
        "    for epoch in range(start_epoch, max_epochs):\n",
        "        print(time.ctime(), \"Epoch:\", epoch)\n",
        "        epoch_time = time.time()\n",
        "        train_acc1, train_acc2,  train_acc4, train_acc5, train_acc6, train_loss, train_accur = train_epoch( model,train_loader,optimizer,epoch=epoch,nclass=nclass,batch_size=batch_size,max_epochs=max_epochs,)\n",
        "        #print(\n",
        "        #    \"Final training  {}/{}\".format(epoch, max_epochs - 1),\n",
        "        #    \"loss: {:.4f}\".format(train_loss),\n",
        "        #    \"time {:.2f}s\".format(time.time() - epoch_time),\n",
        "        #)\n",
        "\n",
        "        if (epoch + 1) % val_every == 0 or epoch == 0:\n",
        "            loss_epochs.append(train_loss)\n",
        "            trains_epoch.append(int(epoch))\n",
        "            epoch_time = time.time()\n",
        "            val_acc1, val_acc2,  val_acc4, val_acc5, val_acc6, val_loss, accur = val_epoch(\n",
        "                model,\n",
        "                val_loader,\n",
        "                epoch=epoch,\n",
        "                nclass=nclass,\n",
        "                max_epochs=max_epochs,\n",
        "                post_label=post_label,\n",
        "                batch_size=batch_size,\n",
        "            )\n",
        "            f1 = val_acc1\n",
        "            roc = val_acc2\n",
        "            #print(\n",
        "             #   \"Final validation stats {}/{}\".format(epoch, max_epochs - 1),\n",
        "             ##   \", Val_loss:\",\n",
        "             #   val_loss,\n",
        "             #    \", Training_loss:\",\n",
        "             #   train_loss,\n",
        "             #   \", time {:.2f}s\".format(time.time() - epoch_time),\n",
        "           # )\n",
        "\n",
        "\n",
        "            val_loss_tracker.append(val_loss)\n",
        "            f1_tracker.append(f1)\n",
        "            roc_tracker.append(roc)\n",
        "            accur_tracker.append(accur)\n",
        "            sens_tracker.append(val_acc5)\n",
        "            prec_tracker.append(val_acc6)\n",
        "            state=False\n",
        "            patient+=1\n",
        "            patient2+=1\n",
        "\n",
        "\n",
        "             # This analysis runs both Loss Earley Stop (LES) and the DMMT with different patient numbers. The patient number is related with the LES patient "patient" and the DMMT delta_t moving average computing "patient2" \n",
        "\n",
        "            case1=False\n",
        "            case2=False\n",
        "            case3=False\n",
        "            case4=False\n",
        "            case5=False\n",
        "            case6=False\n",
        "            case7=False\n",
        "            case8=False\n",
        "            case9=False\n",
        "            case10=False\n",
        "            case11=False\n",
        "            case12=False\n",
        "            case13=False\n",
        "            scheduler[0].step()\n",
        "            wandb.log({\"train_loss\": train_loss, \"val_loss\":val_loss, \"train_f1_score\": train_acc1, \"train_ROC\": train_acc2, \"train_specificity\":  train_acc4,\"train_sensitivity\": train_acc5, \"train_precision\": train_acc6, \"train_accuracy\": train_accur,\"f1_score\": f1, \"ROC\": roc, \"specificity\":  val_acc4,\"sensitivity\": val_acc5, \"precision\": val_acc6, \"accuracy\": accur})\n",
        "            if (val_loss<=val_loss_max):\n",
        "               patient=0\n",
        "               val_loss_max = val_loss\n",
        "               loss=train_loss\n",
        "               save_checkpoint(model,epoch,filename=model_name,val_loss=val_loss,train_loss=loss,dir_add=root_dir,)\n",
        "            if LES==True:\n",
        "               if patient==5:\n",
        "                  print('The LES patient 5 was finished in epoch: ',epoch)\n",
        "               elif patient2==5 and case1==False:\n",
        "                  dmmt=DMMT(delta_k=0.02,delta_t=5)\n",
        "                  state, M_p, loss_ =dmmt(loss=val_loss,loss_prev=loss_p,M=[f1_tracker,roc_tracker,accur_tracker,sens_tracker,prec_tracker], Mprev=M_p)\n",
        "                  loss_p=loss_\n",
        "                  if state==False:\n",
        "                      patient2=0\n",
        "                      f1_tracker=[]\n",
        "                      roc_tracker=[]\n",
        "                      accur_tracker=[]\n",
        "                      sens_tracker=[]\n",
        "                      prec_tracker=[]\n",
        "                  else:\n",
        "                      print('The DMMT patient 5 was finished in epoch: ',epoch)\n",
        "                      case1=True\n",
        "\n",
        "               elif patient==10:\n",
        "                  print('The LES patient 10 was finished in epoch: ',epoch)\n",
        "               elif patient2==10 and case2==False:\n",
        "                  dmmt=DMMT(delta_k=0.02,delta_t=10)\n",
        "                  state, M_p, loss_ =dmmt(loss=val_loss,loss_prev=loss_p,M=[f1_tracker,roc_tracker,accur_tracker,sens_tracker,prec_tracker], Mprev=M_p)\n",
        "                  loss_p=loss_\n",
        "                  if state==False:\n",
        "                      patient2=0\n",
        "                      f1_tracker=[]\n",
        "                      roc_tracker=[]\n",
        "                      accur_tracker=[]\n",
        "                      sens_tracker=[]\n",
        "                      prec_tracker=[]\n",
        "                  else:\n",
        "                      print('The DMMT patient 10 was finished in epoch: ',epoch)\n",
        "                      case2=True\n",
        "\n",
        "               elif patient==15:\n",
        "                  print('The LES patient 15 was finished in epoch: ',epoch)\n",
        "               elif patient2==15 and  case3==False:\n",
        "                  dmmt=DMMT(delta_k=0.02,delta_t=15)\n",
        "                  state, M_p, loss_ =dmmt(loss=val_loss,loss_prev=loss_p,M=[f1_tracker,roc_tracker,accur_tracker,sens_tracker,prec_tracker], Mprev=M_p)\n",
        "                  loss_p=loss_\n",
        "                  if state==False:\n",
        "                      patient2=0\n",
        "                      f1_tracker=[]\n",
        "                      roc_tracker=[]\n",
        "                      accur_tracker=[]\n",
        "                      sens_tracker=[]\n",
        "                      prec_tracker=[]\n",
        "                  else:\n",
        "                      print('The DMMT patient 15 was finished in epoch: ',epoch)\n",
        "                      case3=True\n",
        "\n",
        "               elif patient==20:\n",
        "                  print('The LES patient 20 was finished in epoch: ',epoch)\n",
        "               elif patient2==20 and case4==False:\n",
        "                  dmmt=DMMT(delta_k=0.02,delta_t=20)\n",
        "                  state, M_p, loss_ =dmmt(loss=val_loss,loss_prev=loss_p,M=[f1_tracker,roc_tracker,accur_tracker,sens_tracker,prec_tracker], Mprev=M_p)\n",
        "                  loss_p=loss_\n",
        "                  if state==False:\n",
        "                      patient2=0\n",
        "                      f1_tracker=[]\n",
        "                      roc_tracker=[]\n",
        "                      accur_tracker=[]\n",
        "                      sens_tracker=[]\n",
        "                      prec_tracker=[]\n",
        "                  else:\n",
        "                      print('The DMMT patient 20 was finished in epoch: ',epoch)\n",
        "                      case4=True\n",
        "\n",
        "               elif patient==25:\n",
        "                  print('The LES patient 25 was finished in epoch: ',epoch)\n",
        "               elif patient2==25 and  case5==False:\n",
        "                  dmmt=DMMT(delta_k=0.02,delta_t=25)\n",
        "                  state, M_p, loss_ =dmmt(loss=val_loss,loss_prev=loss_p,M=[f1_tracker,roc_tracker,accur_tracker,sens_tracker,prec_tracker], Mprev=M_p)\n",
        "                  loss_p=loss_\n",
        "                  if state==False:\n",
        "                      patient2=0\n",
        "                      f1_tracker=[]\n",
        "                      roc_tracker=[]\n",
        "                      accur_tracker=[]\n",
        "                      sens_tracker=[]\n",
        "                      prec_tracker=[]\n",
        "                  else:\n",
        "                      print('The DMMT patient 25 was finished in epoch: ',epoch)\n",
        "                      case5=True\n",
        "\n",
        "               elif patient==30:\n",
        "                  print('The LES patient 30 was finished in epoch: ',epoch)\n",
        "               elif patient2==30 and  case6==False:\n",
        "                  dmmt=DMMT(delta_k=0.02,delta_t=30)\n",
        "                  state, M_p, loss_ =dmmt(loss=val_loss,loss_prev=loss_p,M=[f1_tracker,roc_tracker,accur_tracker,sens_tracker,prec_tracker], Mprev=M_p)\n",
        "                  loss_p=loss_\n",
        "                  if state==False:\n",
        "                      patient2=0\n",
        "                      f1_tracker=[]\n",
        "                      roc_tracker=[]\n",
        "                      accur_tracker=[]\n",
        "                      sens_tracker=[]\n",
        "                      prec_tracker=[]\n",
        "                  else:\n",
        "                      print('The DMMT patient 30 was finished in epoch: ',epoch)\n",
        "                      case6=True\n",
        "\n",
        "               elif patient==40:\n",
        "                  print('The LES patient 40 was finished in epoch: ',epoch)\n",
        "               elif patient2==40 and  case7==False:\n",
        "                  dmmt=DMMT(delta_k=0.02,delta_t=40)\n",
        "                  state, M_p, loss_ =dmmt(loss=val_loss,loss_prev=loss_p,M=[f1_tracker,roc_tracker,accur_tracker,sens_tracker,prec_tracker], Mprev=M_p)\n",
        "                  loss_p=loss_\n",
        "                  if state==False:\n",
        "                      patient2=0\n",
        "                      f1_tracker=[]\n",
        "                      roc_tracker=[]\n",
        "                      accur_tracker=[]\n",
        "                      sens_tracker=[]\n",
        "                      prec_tracker=[]\n",
        "                  else:\n",
        "                      print('The DMMT patient 40 was finished in epoch: ',epoch)\n",
        "                      case7=True\n",
        "\n",
        "               elif patient==50:\n",
        "                  print('The LES patient 50 was finished in epoch: ',epoch)\n",
        "               elif patient2==50 and  case8==False:\n",
        "                  dmmt=DMMT(delta_k=0.02,delta_t=50)\n",
        "                  state, M_p, loss_ =dmmt(loss=val_loss,loss_prev=loss_p,M=[f1_tracker,roc_tracker,accur_tracker,sens_tracker,prec_tracker], Mprev=M_p)\n",
        "                  loss_p=loss_\n",
        "                  if state==False:\n",
        "                      patient2=0\n",
        "                      f1_tracker=[]\n",
        "                      roc_tracker=[]\n",
        "                      accur_tracker=[]\n",
        "                      sens_tracker=[]\n",
        "                      prec_tracker=[]\n",
        "                  else:\n",
        "                      print('The DMMT patient 50 was finished in epoch: ',epoch)\n",
        "                      case8=True\n",
        "\n",
        "               elif patient==60:\n",
        "                  print('The LES patient 60 was finished in epoch: ',epoch)\n",
        "               elif patient2==60 and  case9==False:\n",
        "                  dmmt=DMMT(delta_k=0.02,delta_t=60)\n",
        "                  state, M_p, loss_ =dmmt(loss=val_loss,loss_prev=loss_p,M=[f1_tracker,roc_tracker,accur_tracker,sens_tracker,prec_tracker], Mprev=M_p)\n",
        "                  loss_p=loss_\n",
        "                  if state==False:\n",
        "                      patient2=0\n",
        "                      f1_tracker=[]\n",
        "                      roc_tracker=[]\n",
        "                      accur_tracker=[]\n",
        "                      sens_tracker=[]\n",
        "                      prec_tracker=[]\n",
        "                  else:\n",
        "                      print('The DMMT patient 60 was finished in epoch: ',epoch)\n",
        "                      case9=True\n",
        "\n",
        "               elif patient==70:\n",
        "                  print('The LES patient 70 was finished in epoch: ',epoch)\n",
        "               elif patient2==70 and  case10==False:\n",
        "                  dmmt=DMMT(delta_k=0.02,delta_t=70)\n",
        "                  state, M_p, loss_ =dmmt(loss=val_loss,loss_prev=loss_p,M=[f1_tracker,roc_tracker,accur_tracker,sens_tracker,prec_tracker], Mprev=M_p)\n",
        "                  loss_p=loss_\n",
        "                  if state==False:\n",
        "                      patient2=0\n",
        "                      f1_tracker=[]\n",
        "                      roc_tracker=[]\n",
        "                      accur_tracker=[]\n",
        "                      sens_tracker=[]\n",
        "                      prec_tracker=[]\n",
        "                  else:\n",
        "                      print('The DMMT patient 70 was finished in epoch: ',epoch)\n",
        "                      case10=True\n",
        "\n",
        "               elif patient==80:\n",
        "                  print('The LES patient 80 was finished in epoch: ',epoch)\n",
        "               elif patient2==80 and  case11==False:\n",
        "                  dmmt=DMMT(delta_k=0.02,delta_t=80)\n",
        "                  state, M_p, loss_ =dmmt(loss=val_loss,loss_prev=loss_p,M=[f1_tracker,roc_tracker,accur_tracker,sens_tracker,prec_tracker], Mprev=M_p)\n",
        "                  loss_p=loss_\n",
        "                  if state==False:\n",
        "                      patient2=0\n",
        "                      f1_tracker=[]\n",
        "                      roc_tracker=[]\n",
        "                      accur_tracker=[]\n",
        "                      sens_tracker=[]\n",
        "                      prec_tracker=[]\n",
        "                  else:\n",
        "                      print('The DMMT patient 80 was finished in epoch: ',epoch)\n",
        "                      case11=True\n",
        "\n",
        "               elif patient==90:\n",
        "                  print('The LES patient 90 was finished in epoch: ',epoch)\n",
        "               elif patient2==90 and  case12==False:\n",
        "                  dmmt=DMMT(delta_k=0.02,delta_t=90)\n",
        "                  state, M_p, loss_ =dmmt(loss=val_loss,loss_prev=loss_p,M=[f1_tracker,roc_tracker,accur_tracker,sens_tracker,prec_tracker], Mprev=M_p)\n",
        "                  loss_p=loss_\n",
        "                  if state==False:\n",
        "                      patient2=0\n",
        "                      f1_tracker=[]\n",
        "                      roc_tracker=[]\n",
        "                      accur_tracker=[]\n",
        "                      sens_tracker=[]\n",
        "                      prec_tracker=[]\n",
        "                  else:\n",
        "                      print('The DMMT patient 90 was finished in epoch: ',epoch)\n",
        "                      case12=True\n",
        "\n",
        "               elif patient==100:\n",
        "                  print('The LES patient 100 was finished in epoch: ',epoch)\n",
        "               elif patient2==100 and  case13==False:\n",
        "                  dmmt=DMMT(delta_k=0.02,delta_t=100)\n",
        "                  state, M_p, loss_ =dmmt(loss=val_loss,loss_prev=loss_p,M=[f1_tracker,roc_tracker,accur_tracker,sens_tracker,prec_tracker], Mprev=M_p)\n",
        "                  loss_p=loss_\n",
        "                  if state==False:\n",
        "                      patient2=0\n",
        "                      f1_tracker=[]\n",
        "                      roc_tracker=[]\n",
        "                      accur_tracker=[]\n",
        "                      sens_tracker=[]\n",
        "                      prec_tracker=[]\n",
        "                  else:\n",
        "                      print('The DMMT patient 100 was finished in epoch: ',epoch)\n",
        "                      case13=True\n",
        "               else:\n",
        "                  print('epoch test: ',epoch)\n",
        "    print(\"Training Finished !, Best Accuracy: \", val_loss)\n",
        "    return ( train_loss,val_loss,f1,roc,val_acc5,val_acc6,accur)\n"
      ],
      "metadata": {
        "id": "uQUdiaIcBiNk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "#from monai.networks.nets import SwinUNETR\n",
        "import monai\n",
        "from monai.networks.nets import Discriminator, Generator\n",
        "\n",
        "class networks:\n",
        "    def __init__(self,name,device):\n",
        "        self.net=name\n",
        "        self.device=device\n",
        "\n",
        "    def build(self,c=2):\n",
        "        if self.net=='resnet50':\n",
        "            net=monai.networks.nets.resnet50(pretrained=False,spatial_dims=2,n_input_channels=3,num_classes=c).to(self.device)\n",
        "        elif self.net=='resnet34':\n",
        "            net=monai.networks.nets.resnet34(pretrained=True,spatial_dims=2,n_input_channels=3,num_classes=c).to(self.device)\n",
        "        elif self.net=='efficient':\n",
        "            net=monai.networks.nets.EfficientNetBN(model_name='efficientnet-l2',spatial_dims=3, in_channels=1,num_classes=c).to(self.device)\n",
        "        elif self.net=='densenet121':\n",
        "            net=monai.networks.nets.DenseNet121(spatial_dims=2, in_channels=3, out_channels=c).to(self.device)\n",
        "        elif self.net=='seresnext50':\n",
        "            net=monai.networks.nets.SEResNext50(spatial_dims=2, in_channels=3, out_channels=c).to(self.device)\n",
        "        elif self.net=='highresnet50':\n",
        "            net=monai.networks.nets.HighResNet(spatial_dims=2, in_channels=3, out_channels=c).to(self.device)\n",
        "        else:\n",
        "            print('nodefine network!!')\n",
        "\n",
        "        return net\n"
      ],
      "metadata": {
        "id": "9eismWwlALg5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import wandb\n",
        "\n",
        "\n",
        "\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "nclass =10\n",
        "torch.backends.cudnn.benchmark = True\n",
        "batch_n = 1024\n",
        "shapex=12\n",
        "end_point=10\n",
        "LES=True\n",
        "trainset = torchvision.datasets.CIFAR10(root='./data', train=True,download=True, transform=transform)\n",
        "trainloader = torch.utils.data.DataLoader(trainset, batch_size=batch_n,shuffle=True, num_workers=2)\n",
        "testset = torchvision.datasets.CIFAR10(root='./data', train=False,download=True, transform=transform)\n",
        "validloader = torch.utils.data.DataLoader(testset, batch_size=batch_n,shuffle=False, num_workers=2)\n",
        "train_loader, val_loader= trainloader,  validloader\n",
        "\n",
        "num_epochs = 800\n",
        "wandb.login(key=\"5d50fdf4c31cf52cf6e5789c6a26b93311d1648c\")\n",
        "run = wandb.init(project=\"paper_DMMT\",config={\"learning_rate\": 5e-4,\"epochs\": num_epochs, \"batch\" : batch_n })\n",
        "model_na='resnet50'\n",
        "net2=networks(model_na,device)\n",
        "net=net2.build(c=nclass)\n",
        "model_name=\"resnet50all.pt\"\n",
        "post_label = AsDiscrete(argmax=False, to_onehot=nclass)\n",
        "optimizer = torch.optim.AdamW(net.parameters(), lr=5e-4, weight_decay=1e-5)\n",
        "scheduler =torch.optim.lr_scheduler.StepLR(optimizer, step_size=num_epochs, gamma=0.1)\n",
        "#scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, T_max=num_epochs)\n",
        "start_epoch = 0\n",
        "(        train_loss,\n",
        "        val_loss,\n",
        "        f1,\n",
        "        roc,\n",
        "        val_acc4,\n",
        "        val_acc6,accur,\n",
        ") = trainer(\n",
        "    model=[net],\n",
        "    train_loader=train_loader,\n",
        "    val_loader=val_loader,\n",
        "    optimizer=[optimizer],\n",
        "    scheduler=[scheduler],\n",
        "    start_epoch=start_epoch,\n",
        "    post_label=post_label,\n",
        "    nclass=nclass,\n",
        "    batch_size=batch_n,\n",
        "    max_epochs=num_epochs,\n",
        "    model_name=model_name,\n",
        "    end_point=end_point,\n",
        "    LES=LES,\n",
        "    )\n",
        "print('train_loss: ',\n",
        "        train_loss, ' validation_loss: ',\n",
        "        val_loss, ' fi_score: ',\n",
        "        f1,  ' ROC: ',\n",
        "        roc, ' sensitivity: ',\n",
        "        val_acc4, ' precision: ',\n",
        "        val_acc6, ' accuracy: ', accur,)"
      ],
      "metadata": {
        "id": "CVamtQydAmjh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "XAI print based on quantus"
      ],
      "metadata": {
        "id": "7GVwcN8CNr52"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "IBoNzJDrx0js"
      },
      "execution_count": null,
      "outputs": []
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuClass": "premium",
      "gpuType": "V100",
      "machine_shape": "hm"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
